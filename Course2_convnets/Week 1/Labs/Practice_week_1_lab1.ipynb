{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Using more sophisticated images with Convolutional Neural Networks"
      ],
      "metadata": {
        "id": "n27q3uFMG1DN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Getting the dataset"
      ],
      "metadata": {
        "id": "vytcNkrdHJgt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-iyJz80bGseL"
      },
      "outputs": [],
      "source": [
        "!wget --no-check-certificate https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Importing zipfile"
      ],
      "metadata": {
        "id": "FqQ3g9FpHKj6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "\n",
        "local_zip = './cats_and_dogs_filtered.zip'\n",
        "zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
        "zip_ref.extractall()\n",
        "\n",
        "zip_ref.close()"
      ],
      "metadata": {
        "id": "M9l6D1M5HUCD"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can assign each of these directories to a variable so you can use it later.\n"
      ],
      "metadata": {
        "id": "54Zr5F8OHq-L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "#Base directory\n",
        "base_dir = 'cats_and_dogs_filtered'\n",
        "\n",
        "train_dir = os.path.join(base_dir, 'train')\n",
        "validation_dir = os.path.join(base_dir, 'validation')\n",
        "\n",
        "#Directory with training cat/dog pictures\n",
        "train_cats_dir = os.path.join(train_dir, 'cats')\n",
        "train_dogs_dir = os.path.join(train_dir, 'dogs')\n",
        "\n",
        "#Directory with validation cat/dog pictures\n",
        "validation_cats_dir = os.path.join(validation_dir, ' cats')\n",
        "validation_dogs_dir = os.path.join(validation_dir, 'dogs')"
      ],
      "metadata": {
        "id": "hJyU3752HsFT"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We also can see what the filenames look like in the `cats` and `dogs` `train` directories (file naming conventions are the same in the `validation` directory)."
      ],
      "metadata": {
        "id": "WL7-KhWeIcJV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_cat_fnames = os.listdir(train_cats_dir)\n",
        "train_dog_fnames = os.listdir(train_dogs_dir)\n",
        "\n",
        "print(train_cat_fnames[:10])\n",
        "print(train_dog_fnames[:10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p11gLQmHIfob",
        "outputId": "f33e36dd-bf83-41dd-a197-dc4911ce74c3"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['cat.432.jpg', 'cat.800.jpg', 'cat.683.jpg', 'cat.550.jpg', 'cat.227.jpg', 'cat.191.jpg', 'cat.915.jpg', 'cat.266.jpg', 'cat.151.jpg', 'cat.846.jpg']\n",
            "['dog.401.jpg', 'dog.126.jpg', 'dog.533.jpg', 'dog.890.jpg', 'dog.578.jpg', 'dog.693.jpg', 'dog.38.jpg', 'dog.889.jpg', 'dog.784.jpg', 'dog.313.jpg']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Building a Small Model from Scratch to get to ~72% Accuracy\n"
      ],
      "metadata": {
        "id": "KkKZBGDfTDXT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We add the layers."
      ],
      "metadata": {
        "id": "G3-d1lf7TyhC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "model = tf.keras.models.Sequential([\n",
        "    #three sets of convolutions + pooling\n",
        "    tf.keras.layers.Conv2D(16, (3,3), activation = 'relu', input_shape = (150,150,3)),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    tf.keras.layers.Conv2D(32, (3,3), activation = 'relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    tf.keras.layers.Conv2D(64, (3,3), activation = 'relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(512, activation = 'relu'),\n",
        "    tf.keras.layers.Dense(1, activation = 'sigmoid')\n",
        "])"
      ],
      "metadata": {
        "id": "u4dx9KoWTEC8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We conpile the model, configuring the specifications for model training."
      ],
      "metadata": {
        "id": "6ijKNAfUT1XJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.optimizers import RMSprop\n",
        "\n",
        "model.compile(\n",
        "    optimizer = RMSprop(learning_rate = 0.001),\n",
        "    loss = 'binary_crossentropy',\n",
        "    metrics = ['accuracy']\n",
        ")"
      ],
      "metadata": {
        "id": "wtLxqhitT6t8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Data Preprocessing"
      ],
      "metadata": {
        "id": "WtbwV7YFUO_Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We set up the data generators (one for training images and one for validation images)\n",
        "- Read pics in the course folders\n",
        "- Convert them to `float32` tensors\n",
        "- Feed them (with their labels) to the model"
      ],
      "metadata": {
        "id": "15_GlRZzUT5K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "#Rescale all images by 1./255\n",
        "train_datagen = ImageDataGenerator(rescale = 1.0/255.)\n",
        "validation_datagen = ImageDataGenerator(rescale = 1.0/255)\n",
        "\n",
        "#Flow training images in batches of 20 using train_datagen generator\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    train_dir,\n",
        "    batch_size = 20,\n",
        "    class_mode = 'binary',\n",
        "    target_size = (150,150)\n",
        ")\n",
        "\n",
        "#Flow validation images in batches of 20 using test_datagen generator\n",
        "validation_generator = validation_datagen.flow_from_directory(\n",
        "    validation_dir,\n",
        "    batch_size = 20,\n",
        "    class_mode = 'binary',\n",
        "    target_size = (150,150)\n",
        ")"
      ],
      "metadata": {
        "id": "_AXQXS9eUP8d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Training the model"
      ],
      "metadata": {
        "id": "8wwdQur-VM-T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(\n",
        "    train_generator,\n",
        "    epochs = 15,\n",
        "    validation_data = validation_generator,\n",
        "    verbose = 2\n",
        ")"
      ],
      "metadata": {
        "id": "VojlQ0cgVP87"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Model Prediction"
      ],
      "metadata": {
        "id": "Yci24Js1VXH2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from google.colab import files\n",
        "from tensorflow.keras.utils import load_img, img_to_array\n",
        "\n",
        "uploaded=files.upload()\n",
        "\n",
        "for fn in uploaded.keys():\n",
        "\n",
        "  # predicting images\n",
        "  path='/content/' + fn\n",
        "  img=load_img(path, target_size=(150, 150))\n",
        "\n",
        "  x=img_to_array(img)\n",
        "  x /= 255\n",
        "  x=np.expand_dims(x, axis=0)\n",
        "  images = np.vstack([x])\n",
        "\n",
        "  classes = model.predict(images, batch_size=10)\n",
        "\n",
        "  print(classes[0])\n",
        "\n",
        "  if classes[0]>0.5:\n",
        "    print(fn + \" is a dog\")\n",
        "  else:\n",
        "    print(fn + \" is a cat\")"
      ],
      "metadata": {
        "id": "jD5gpyCGVYCW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Visualization"
      ],
      "metadata": {
        "id": "ybCwCMa0VndQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*optional"
      ],
      "metadata": {
        "id": "FR0IpZngVzd0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import random\n",
        "from tensorflow.keras.utils import img_to_array, load_img\n",
        "\n",
        "# Define a new Model that will take an image as input, and will output\n",
        "# intermediate representations for all layers in the previous model\n",
        "successive_outputs = [layer.output for layer in model.layers]\n",
        "visualization_model = tf.keras.models.Model(inputs = model.input, outputs = successive_outputs)\n",
        "\n",
        "# Prepare a random input image from the training set.\n",
        "cat_img_files = [os.path.join(train_cats_dir, f) for f in train_cat_fnames]\n",
        "dog_img_files = [os.path.join(train_dogs_dir, f) for f in train_dog_fnames]\n",
        "img_path = random.choice(cat_img_files + dog_img_files)\n",
        "img = load_img(img_path, target_size=(150, 150))  # this is a PIL image\n",
        "x   = img_to_array(img)                           # Numpy array with shape (150, 150, 3)\n",
        "x   = x.reshape((1,) + x.shape)                   # Numpy array with shape (1, 150, 150, 3)\n",
        "\n",
        "# Scale by 1/255\n",
        "x /= 255.0\n",
        "\n",
        "# Run the image through the network, thus obtaining all\n",
        "# intermediate representations for this image.\n",
        "successive_feature_maps = visualization_model.predict(x)\n",
        "\n",
        "# These are the names of the layers, so you can have them as part of our plot\n",
        "layer_names = [layer.name for layer in model.layers]\n",
        "\n",
        "# Display the representations\n",
        "for layer_name, feature_map in zip(layer_names, successive_feature_maps):\n",
        "\n",
        "  if len(feature_map.shape) == 4:\n",
        "\n",
        "    #-------------------------------------------\n",
        "    # Just do this for the conv / maxpool layers, not the fully-connected layers\n",
        "    #-------------------------------------------\n",
        "    n_features = feature_map.shape[-1]  # number of features in the feature map\n",
        "    size       = feature_map.shape[ 1]  # feature map shape (1, size, size, n_features)\n",
        "\n",
        "    # Tile the images in this matrix\n",
        "    display_grid = np.zeros((size, size * n_features))\n",
        "\n",
        "    #-------------------------------------------------\n",
        "    # Postprocess the feature to be visually palatable\n",
        "    #-------------------------------------------------\n",
        "    for i in range(n_features):\n",
        "      x  = feature_map[0, :, :, i]\n",
        "      x -= x.mean()\n",
        "      x /= x.std ()\n",
        "      x *=  64\n",
        "      x += 128\n",
        "      x  = np.clip(x, 0, 255).astype('uint8')\n",
        "      display_grid[:, i * size : (i + 1) * size] = x # Tile each filter into a horizontal grid\n",
        "\n",
        "    #-----------------\n",
        "    # Display the grid\n",
        "    #-----------------\n",
        "    scale = 20. / n_features\n",
        "    plt.figure( figsize=(scale * n_features, scale) )\n",
        "    plt.title ( layer_name )\n",
        "    plt.grid  ( False )\n",
        "    plt.imshow( display_grid, aspect='auto', cmap='viridis' )"
      ],
      "metadata": {
        "id": "RN4dgZeVVpYs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Evaluating accuracy and loss for the model"
      ],
      "metadata": {
        "id": "APbTk8KXVwiL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*optional"
      ],
      "metadata": {
        "id": "vlaEhVqkV0vA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#-----------------------------------------------------------\n",
        "# Retrieve a list of list results on training and test data\n",
        "# sets for each training epoch\n",
        "#-----------------------------------------------------------\n",
        "acc      = history.history[     'accuracy' ]\n",
        "val_acc  = history.history[ 'val_accuracy' ]\n",
        "loss     = history.history[    'loss' ]\n",
        "val_loss = history.history['val_loss' ]\n",
        "\n",
        "epochs   = range(len(acc)) # Get number of epochs\n",
        "\n",
        "#------------------------------------------------\n",
        "# Plot training and validation accuracy per epoch\n",
        "#------------------------------------------------\n",
        "plt.plot  ( epochs,     acc )\n",
        "plt.plot  ( epochs, val_acc )\n",
        "plt.title ('Training and validation accuracy')\n",
        "plt.figure()\n",
        "\n",
        "#------------------------------------------------\n",
        "# Plot training and validation loss per epoch\n",
        "#------------------------------------------------\n",
        "plt.plot  ( epochs,     loss )\n",
        "plt.plot  ( epochs, val_loss )\n",
        "plt.title ('Training and validation loss'   )"
      ],
      "metadata": {
        "id": "uDI9TkG8VyD5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}