{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Ungraded Lab: Training with ImageDataGenerator\n"
      ],
      "metadata": {
        "id": "b3cqB9XhUrfB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://storage.googleapis.com/tensorflow-1-public/course2/week3/horse-or-human.zip"
      ],
      "metadata": {
        "id": "B25NUxCqAy2d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "\n",
        "#unzip the dataset\n",
        "local_zip = './horse-or-human.zip'\n",
        "zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
        "zip_ref.extractall('./horse-or-human')\n",
        "zip_ref.close()"
      ],
      "metadata": {
        "id": "DhmKjgg_Eprv"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Defining the directories\n",
        "\n",
        "import os\n",
        "\n",
        "#Directory with our training horse pictures\n",
        "train_horse_dir = os.path.join('./horse-or-human/horses')\n",
        "\n",
        "#Directory with our training human pictures\n",
        "train_human_dir = os.path.join('./horse-or-human/humans')"
      ],
      "metadata": {
        "id": "Gvehena9VDXb"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#We can see what the filenames look like in the horses and humans training directories\n",
        "\n",
        "train_horses_names = os.listdir(train_horse_dir)\n",
        "print(train_horses_names[:10])\n",
        "\n",
        "train_human_names = os.listdir(train_human_dir)\n",
        "print(train_human_names[:10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O6z6d2a2VbiB",
        "outputId": "bc77b2e9-714c-4258-f7dd-0bd28f00a70b"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['horse40-0.png', 'horse13-3.png', 'horse29-8.png', 'horse40-2.png', 'horse18-9.png', 'horse13-1.png', 'horse31-1.png', 'horse28-5.png', 'horse41-4.png', 'horse01-6.png']\n",
            "['human06-11.png', 'human12-22.png', 'human14-29.png', 'human16-13.png', 'human13-23.png', 'human08-26.png', 'human10-27.png', 'human12-15.png', 'human01-28.png', 'human02-27.png']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Building a Small Model from Scratch"
      ],
      "metadata": {
        "id": "2JepMblHWKv4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "qGp1dCOzWOi6"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.models.Sequential([\n",
        "    # Note the input shape is the desired size of the image 300x300 with 3 bytes color\n",
        "\n",
        "    # This is the first convolution\n",
        "    tf.keras.layers.Conv2D(16, (3,3), activation='relu', input_shape=(300, 300, 3)),\n",
        "    tf.keras.layers.MaxPooling2D(2, 2),\n",
        "    # The second convolution\n",
        "    tf.keras.layers.Conv2D(32, (3,3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    # The third convolution\n",
        "    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    # The fourth convolution\n",
        "    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    # The fifth convolution\n",
        "    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "\n",
        "    # Flatten the results to feed into a DNN\n",
        "    tf.keras.layers.Flatten(),\n",
        "    # 512 neuron hidden layer\n",
        "    tf.keras.layers.Dense(512, activation='relu'),\n",
        "    # Only 1 output neuron. It will contain a value from 0-1 where 0 for 1 class ('horses') and 1 for the other ('humans')\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])"
      ],
      "metadata": {
        "id": "tpzr67BYWQUJ"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "WE49wiYdWZJ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.optimizers import RMSprop"
      ],
      "metadata": {
        "id": "JF6hGCcIWabG"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(\n",
        "    loss = 'binary_crossentropy',\n",
        "    optimizer = RMSprop(learning_rate = 0.001),\n",
        "    metrics = ['accuracy']\n",
        ")"
      ],
      "metadata": {
        "id": "lIguC7eNWeNE"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Data Preprocessing"
      ],
      "metadata": {
        "id": "JHDuFxvoWwQf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We read the pictures in the source folders, convert them to `float32` tensors, and feed them (with their labels) to the model."
      ],
      "metadata": {
        "id": "X4uM3X3EW2Ph"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "#All images rescaled by 1/255\n",
        "train_datagen = ImageDataGenerator(rescale = 1/255)\n",
        "\n",
        "#Flow training images in batches of 128 using train_datagen generator\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    './horse-or-human/',\n",
        "    target_size = (300,300),\n",
        "    batch_size = 128,\n",
        "    class_mode = 'binary'\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y2OfpDnwWzaR",
        "outputId": "cb2fc028-787f-4c44-9466-9cc93f829dab"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 1027 images belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Training"
      ],
      "metadata": {
        "id": "QlT4NABhXcV-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(\n",
        "    train_generator,\n",
        "    steps_per_epoch = 8,\n",
        "    epochs = 15,\n",
        "    verbose = 1\n",
        ")"
      ],
      "metadata": {
        "id": "Tos4jKB2Xddx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Model Prediction\n",
        "This code will allow you to choose 1 or more files from your file system, upload them, and run them through the model, giving an indication of whether the object is a horse or a human."
      ],
      "metadata": {
        "id": "eKXMNlRMYSzE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from google.colab import files\n",
        "from tensorflow.keras.utils import load_img, img_to_array\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "for fn in uploaded.keys():\n",
        "\n",
        "  #predicting images\n",
        "  path = '/content/' + fn\n",
        "  img = load_img(path, target_size = (300,300))\n",
        "  x = img_to_array(img)\n",
        "  x /= 255\n",
        "  x = np.expand_dims(x, axis = 0)\n",
        "\n",
        "  images = np.vstack([x])\n",
        "  classes = model.predict(images, batch_size = 10)\n",
        "  print(classes[0])\n",
        "\n",
        "  if classes[0]>0.5:\n",
        "    print(fn + \"is a human\")\n",
        "  else:\n",
        "    print(fn + \"is a horse\")"
      ],
      "metadata": {
        "id": "j5IKi-UgYWRC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}