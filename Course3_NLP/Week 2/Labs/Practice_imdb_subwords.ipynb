{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X51g_qU10AQ3"
      },
      "source": [
        "#Subword Tokenization with the IMDB Reviews Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "htPhmL930Leo"
      },
      "source": [
        "##Download the IMDB reviews\n",
        "- plain text\n",
        "- subwords8k"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "zixbdsfY0Cyn"
      },
      "outputs": [],
      "source": [
        "import tensorflow_datasets as tfds\n",
        "\n",
        "#Downloading plain_text default config\n",
        "imdb_plaintext, info_plaintext = tfds.load(\"imdb_reviews\", with_info = True, as_supervised = True)\n",
        "\n",
        "#Doanloading subword encoded pretokenized dataset\n",
        "imdb_subwords, info_subwords = tfds.load(\"imdb_reiews/subwords8k\", with_info = )"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Subword Text Encoding"
      ],
      "metadata": {
        "id": "65NStJbB1BTd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We first see the process with plaintext."
      ],
      "metadata": {
        "id": "6U_ZiVPZ1dGC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the train set\n",
        "train_data = imdb_plaintext['train']\n",
        "# Initialize sentences list\n",
        "training_sentences = []\n",
        "# Loop over all training examples and save to the list\n",
        "for s,_ in train_data:\n",
        "  training_sentences.append(s.numpy().decode('utf8'))"
      ],
      "metadata": {
        "id": "1Kw_IkF_1cU-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "vocab_size = 10000\n",
        "oov_tok = '<OOV>'\n",
        "\n",
        "#Initialize the Tokenizer class\n",
        "tokenizer_plaintext = Tokenizer(num_words = 10000, oov_token = oov_tok)\n",
        "\n",
        "# Generate the word index dictionary for the training sentences\n",
        "tokenizer_plaintext.fit_on_texts(training_sentences)\n",
        "\n",
        "# Generate the training sequences\n",
        "sequences = tokenizer_plaintext.texts_to_sequences(training_sentences)"
      ],
      "metadata": {
        "id": "AGG1WoBO1vbb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- If we use this, there will be lots of OOV tokens when decoding using the lookup dictionary it created.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "RVcDoOsB2TTe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W450SJ6yztFe"
      },
      "outputs": [],
      "source": [
        "sample_string = 'Tensorflow, from basics to mastery'\n",
        "\n",
        "tokenized_string = tokenizer_plaintext.texts_to_sequences([sample_string])\n",
        "original_string = tokenizer_plaintext.sequences_to_texts(tokenized_string)\n",
        "\n",
        "print('Tokenized string is {}'.format(tokenized_string))\n",
        "print('The original string is {}'.format(original_string))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Output:\n",
        "\n",
        "``Tokenized string is [[1, 37, 1, 6, 1]]``\n",
        "\n",
        "``The original string: ['<OOV> from <OOV> to <OOV>']``"
      ],
      "metadata": {
        "id": "oMgnBChP3dhK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Increasing the `vocab_size` will just bloat the model."
      ],
      "metadata": {
        "id": "VROTY94P3O31"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Subword text encoding** gets around this problem by using parts of the word to compose whole words. We can see the process of using subword text encoding by starting with the `encoder` object."
      ],
      "metadata": {
        "id": "WL448qs01QqL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8dRfoz22z_M4"
      },
      "outputs": [],
      "source": [
        "tokenizer_subwords = info_subwords.features['text'].encoder"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_string = tokenizer_subwords.encode(sample_string)\n",
        "original_string = tokenizer_subwords.decode(tokenized_string)\n",
        "\n",
        "print ('Tokenized string is {}'.format(tokenized_string))\n",
        "print ('The original string: {}'.format(original_string))"
      ],
      "metadata": {
        "id": "9J2YtbX436sQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Output:\n",
        "\n",
        "``Tokenized string is [6307, 2327, 4043, 2120, 2, 48, 4249, 4429, 7, 2652, 8050]``\n",
        "\n",
        "``The original string: TensorFlow, from basics to mastery``"
      ],
      "metadata": {
        "id": "Zf4gmZup4GFD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you may notice, the sentence is correctly decoded. The downside is the token sequence is much longer. Instead of only 5 when using word-encoding, you ended up with 11 tokens instead. The mapping for this sentence is shown below:"
      ],
      "metadata": {
        "id": "wE_F0Zez4OmE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Show token to subword mapping:\n",
        "for ts in tokenized_string:\n",
        "  print ('{} ----> {}'.format(ts, tokenizer_subwords.decode([ts])))"
      ],
      "metadata": {
        "id": "vGEUl4_N4HDG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Training the model"
      ],
      "metadata": {
        "id": "B5vcu-qi4TKK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You will now train your model using this pre-tokenized dataset. Since these are already saved as sequences, you can jump straight to making uniform sized arrays for the train and test sets. These are also saved as `tf.data.Dataset` type so you can use the padded_batch() method to create batches and pad the arrays into a uniform size for training."
      ],
      "metadata": {
        "id": "y_lZnGum4868"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BUFFER_SIZE = 1000\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# Get the train and test splits\n",
        "train_data, test_data = imdb_subwords['train'], imdb_subwords['test'],\n",
        "\n",
        "#Shuffle the training data\n",
        "train_dataset = train_data.shuffle(BUFFER_SIZE)\n",
        "\n",
        "#Batch and pad the datasets to the maximum length of the sequences\n",
        "train_dataset = train_dataset.padded_batch(BATCH_SIZE)\n",
        "test_dataset = train_data.padded_batch(BATCH_SIZE)"
      ],
      "metadata": {
        "id": "ydwsst784WLb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We then build the model."
      ],
      "metadata": {
        "id": "WMZc4VGq5Jvg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Define dimensionality of the embedding\n",
        "embedding_dim = 64\n",
        "\n",
        "# Build the model\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(tokenizer_subwords.vocab_size, embedding_dim),\n",
        "    tf.keras.layers.GlobalAveragePooling1D(),\n",
        "    tf.keras.layers.Dense(6, activation = 'relu'),\n",
        "    tf.keras.layers.Dense(1, activation = 'sigmoid')\n",
        "])\n",
        "\n",
        "# Print the model summary\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "BjS2utE95K5u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 10\n",
        "\n",
        "# Set the training parameters\n",
        "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
        "\n",
        "# Start training\n",
        "history = model.fit(train_dataset, epochs=num_epochs, validation_data=test_dataset)"
      ],
      "metadata": {
        "id": "-9aVMLVN5b_h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Visualizing the results"
      ],
      "metadata": {
        "id": "G1L_zkqr5dm_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#plot utility\n",
        "def plot_graphs(history, string):\n",
        "  plt.plot(history.history[string])\n",
        "  plt.plot(history.history['val_'+string])\n",
        "  plt.xlabel(\"Epochs\")\n",
        "  plt.ylabel(string)\n",
        "  plt.legend([string, 'val_'+string])\n",
        "  plt.show()\n",
        "\n",
        "#plot the accuracy and results\n",
        "plot_graphs(history, \"accuracy\")\n",
        "plot_graphs(history, \"loss\")"
      ],
      "metadata": {
        "id": "DaxFVdrS5etJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}