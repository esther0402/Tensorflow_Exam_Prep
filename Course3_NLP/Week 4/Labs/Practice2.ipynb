{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Importing the Library"
      ],
      "metadata": {
        "id": "-d67Lzs08Lhv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FrgCtD3A8I9q"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.optimizers import Adam"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Building the Word Vocabulary"
      ],
      "metadata": {
        "id": "Ng7OdlSg8OJ_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the dataset\n",
        "!gdown --id 15UqmiIm0xwh9mt0IYq2z3jHaauxQSTQT"
      ],
      "metadata": {
        "id": "YWD7UCx08Pzq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We then load the dataset."
      ],
      "metadata": {
        "id": "yvBCXz-s8SRr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Load the dataset\n",
        "data = open('./irish-lyrics-eof.txt').read()\n",
        "\n",
        "#Lowercase and split the text\n",
        "corpus = data.lower().split(\"\\n\")"
      ],
      "metadata": {
        "id": "sMwPrExn8RgS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We initialize the `Tokenizer class` and generate the word index dictionary."
      ],
      "metadata": {
        "id": "pxpBecO48fao"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(corpus)\n",
        "total_words = len(tokenizer.word_index) + 1"
      ],
      "metadata": {
        "id": "CotDJFiY8fLH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Preprocessing the Dataset"
      ],
      "metadata": {
        "id": "kdP6X2oW8uWx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is same with the previous lab."
      ],
      "metadata": {
        "id": "P-G4_zbp9UHr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_sequences = []\n",
        "\n",
        "for line in corpus:\n",
        "    #tokenize the current line\n",
        "    token_list = tokenizer.texts_to_sequences([line])[0]\n",
        "\n",
        "    for i in range(1, len(token_list)):\n",
        "      #generate subphrases\n",
        "      n_gram_sequence = token_list[:i+1]\n",
        "      input_sequences.append(n_gram_sequence)\n",
        "\n",
        "    #get the length of the longest line\n",
        "    max_sequence_len = max([len(x) for x in input_sequences])\n",
        "\n",
        "    #pad all sequences\n",
        "    input_sequences = pad_sequences(input_sequences, maxlen = max_sequence_len, padding = 'pre')\n",
        "\n",
        "    #Create  inputs\n",
        "    xs = input_sequences[:,:-1]\n",
        "    labels = input_sequences[:,-1]\n",
        "\n",
        "    #convert label into one hot arrays\n",
        "    ys = tf.keras.utils.to_categorical(labels, num_classes = total_words)"
      ],
      "metadata": {
        "id": "ibQlTy_f8vir"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Build and compile the model"
      ],
      "metadata": {
        "id": "rOem6wQU--YD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Setting hyperparameters\n",
        "embedding_dim = 100\n",
        "lstm_units = 150\n",
        "learning_rate = 0.01\n",
        "\n",
        "#Build the model\n",
        "model = Sequential ([\n",
        "    Embedding(total_words, embedding_dim, input_length = max_seqneuce_len-1),\n",
        "    Bidirectional(LSTM(lstm_units)),\n",
        "    Dense(total_words, activation = 'softmax')\n",
        "])\n",
        "\n",
        "#compile the model\n",
        "model.compile(\n",
        "    loss = 'categorical_crossentropy',\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate = learning_rate),\n",
        "    metrics = ['accuracy']\n",
        ")"
      ],
      "metadata": {
        "id": "2A-0qhdy-_1q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Train the model"
      ],
      "metadata": {
        "id": "Ux22Ooqs_eqS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 100\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(xs, ys, epochs=epochs)"
      ],
      "metadata": {
        "id": "BhXlhMDy_fhY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Visualize the Model"
      ],
      "metadata": {
        "id": "MFx6YimM_hkL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plot utility\n",
        "def plot_graphs(history, string):\n",
        "  plt.plot(history.history[string])\n",
        "  plt.xlabel(\"Epochs\")\n",
        "  plt.ylabel(string)\n",
        "  plt.show()\n",
        "\n",
        "# Visualize the accuracy\n",
        "plot_graphs(history, 'accuracy')"
      ],
      "metadata": {
        "id": "fuXSt0YU_i4U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Generating text"
      ],
      "metadata": {
        "id": "hv9xxVSH_loK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now you can let the model make its own songs or poetry! Because it is trained on a much larger corpus, the results below should contain less repetitions as before. The code below picks the next word based on the highest probability output."
      ],
      "metadata": {
        "id": "vh_bWN8-__7V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define seed text\n",
        "seed_text = \"help me obi-wan kinobi youre my only hope\"\n",
        "\n",
        "#define total words to predict\n",
        "next_words = 100\n",
        "\n",
        "#loop until desired length is reached\n",
        "for _ in range(next_words):\n",
        "  #convert text to token list\n",
        "  token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "\n",
        "  #pad the sequence\n",
        "  token_list = pad_sequences([token_list], maxlen = max_sequence_len-1, padding='pre')\n",
        "\n",
        "  #feed to the model and get the probabilities for each index\n",
        "  probabilities = model.predict(token_list, werbose = 0)\n",
        "\n",
        "  #get the index with the highest probabilitiy\n",
        "  predicted = np.argmax(probabilities, axis = -1)[0]\n",
        "\n",
        "  if predicted != 0:\n",
        "    output_word = tokenizer.index_word[predicted]\n",
        "    seed_text += \" \" + output_word\n",
        "\n",
        "# Print the result\n",
        "print(seed_text)"
      ],
      "metadata": {
        "id": "FXBIwYyu_md9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code gets the top 3 predictions and picks one at random."
      ],
      "metadata": {
        "id": "rMQ9Ed8vB6O0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define seed text\n",
        "seed_text = \"help me obi-wan kinobi youre my only hope\"\n",
        "\n",
        "# Define total words to predict\n",
        "next_words = 100\n",
        "\n",
        "# Loop until desired length is reached\n",
        "for _ in range(next_words):\n",
        "\n",
        "\t# Convert the seed text to a token sequence\n",
        "  token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "\n",
        "\t# Pad the sequence\n",
        "  token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
        "\n",
        "\t# Feed to the model and get the probabilities for each index\n",
        "  probabilities = model.predict(token_list, verbose=0)\n",
        "\n",
        "  # Pick a random number from [1,2,3]\n",
        "  choice = np.random.choice([1,2,3])\n",
        "\n",
        "  # Sort the probabilities in ascending order\n",
        "  # and get the random choice from the end of the array\n",
        "  predicted = np.argsort(probabilities)[0][-choice]\n",
        "\n",
        "\t# Ignore if index is 0 because that is just the padding.\n",
        "  if predicted != 0:\n",
        "\n",
        "\t\t# Look up the word associated with the index.\n",
        "\t  output_word = tokenizer.index_word[predicted]\n",
        "\n",
        "\t\t# Combine with the seed text\n",
        "\t  seed_text += \" \" + output_word\n",
        "\n",
        "# Print the result\n",
        "print(seed_text)"
      ],
      "metadata": {
        "id": "iyu75T1zB9s0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}